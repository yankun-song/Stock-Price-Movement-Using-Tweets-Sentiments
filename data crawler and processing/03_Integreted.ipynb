{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Run the following command in terminal to get the tweets_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsnscrape twitter-search \"#TSLA since:2020-04-01 until:2020-10-10\" > TSLA.txt\\nsnscrape twitter-search \"#AAPL since:2020-04-01 until:2020-10-10\" > AAPL.txt\\nsnscrape twitter-search \"#MSFT since:2020-04-01 until:2020-10-10\" > MSFT.txt\\n\\n\\nsnscrape twitter-search \"#NIO since:2019-11-01 until:2020-11-01\" > NIO.txt\\nsnscrape twitter-search \"#TSLA since:2019-11-01 until:2020-11-01\" > TSLA.txt\\nsnscrape twitter-search \"#AAPL since:2019-11-01 until:2020-11-01\" > AAPL.txt\\nsnscrape twitter-search \"#AAPL since:2019-11-01 until:2020-11-01\" > MSFT.txt\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reference: https://medium.com/@jcldinco/downloading-historical-tweets-using-tweet-ids-via-snscrape-and-tweepy-5f4ecbf19032\n",
    "\"\"\"\n",
    "snscrape twitter-search \"#TSLA since:2020-04-01 until:2020-10-10\" > TSLA.txt\n",
    "snscrape twitter-search \"#AAPL since:2020-04-01 until:2020-10-10\" > AAPL.txt\n",
    "snscrape twitter-search \"#MSFT since:2020-04-01 until:2020-10-10\" > MSFT.txt\n",
    "\n",
    "\n",
    "snscrape twitter-search \"#NIO since:2019-11-01 until:2020-11-01\" > NIO.txt\n",
    "snscrape twitter-search \"#TSLA since:2019-11-01 until:2020-11-01\" > TSLA.txt\n",
    "snscrape twitter-search \"#AAPL since:2019-11-01 until:2020-11-01\" > AAPL.txt\n",
    "snscrape twitter-search \"#AAPL since:2019-11-01 until:2020-11-01\" > MSFT.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Crawl tweets info based on the tweets_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetsCrawler(ticker):\n",
    "    \"\"\"\n",
    "    crawl all the tweets in the specific period, and write them in ticker_tweets.csv\n",
    "    \"\"\"\n",
    "    consumer_key = \"aqL7hyXwXd2Czpn3GNIlWmXNc\" \n",
    "    consumer_secret = \"1JqdHCmidPQnYX4Mu3imGJEcQfpxyEh2z8K5WtgLQCtQ7SBxiQ\" \n",
    "    access_token = \"940903761915949057-OSZIlUYzerbIpXSUZQvYQxmusCSNdl0\" \n",
    "    access_token_secret = \"NMNWR5uNwYAx3QPXQefqDpbbvPbM6MyKRH5xfHuo5tr0E\"\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    tweet_url = pd.read_csv(ticker+\".txt\", index_col= None, header = None, names = [\"links\"])\n",
    "    #tweet_url.head()\n",
    "    \n",
    "    af = lambda x: x[\"links\"].split(\"/\")[-1]\n",
    "    tweet_url['id'] = tweet_url.apply(af, axis=1)\n",
    "    \n",
    "    ids = tweet_url['id'].tolist()\n",
    "    \n",
    "    total_count = len(ids)\n",
    "    chunks = (total_count - 1) // 50 + 1\n",
    "    \n",
    "    def fetch_tw(ids):\n",
    "        list_of_tw_status = api.statuses_lookup(ids, tweet_mode= \"extended\")\n",
    "        empty_data = pd.DataFrame()\n",
    "        for status in list_of_tw_status:\n",
    "                tweet_elem = {\"tweet_id\": status.id,\n",
    "                         \"screen_name\": status.user.screen_name,\n",
    "                        \"followers\": status.user.followers_count,\n",
    "                         \"tweet\":status.full_text,\n",
    "                         \"date\":status.created_at}\n",
    "                empty_data = empty_data.append(tweet_elem, ignore_index = True)\n",
    "        empty_data.to_csv(ticker+\"_tweets.csv\", mode=\"a\")\n",
    "        \n",
    "    for i in range(chunks):\n",
    "        batch = ids[i*50:(i+1)*50]\n",
    "        result = fetch_tw(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyze each day's sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(ticker, weighted):\n",
    "    \"\"\"calculate each day's weighted sentiments, and save the results into a csv file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(ticker+\"_tweets.csv\")\n",
    "    df = df.dropna()    #remove the rows with na values\n",
    "    df = df.reset_index()   # reset the index\n",
    "    df = df.loc[:, ['date', 'followers', 'tweet']]    # only take the key info we'll use\n",
    "    df['date']=pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')    # transfer the type of date\n",
    "    df['followers'] = df['followers'].astype(float)\n",
    "    df['date'] = df['date'].dt.normalize()    # we only need the date\n",
    "    \n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    \n",
    "    def sentiment_scores(sentence):\n",
    "        \"\"\"return a sentiment score of the input sentence\n",
    "        \"\"\"\n",
    "        sid_obj = SentimentIntensityAnalyzer()\n",
    "        sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "        return sentiment_dict['compound']\n",
    "    \n",
    "    df['senti_score'] = 0 \n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        df.loc[i, 'senti_score'] = sentiment_scores(df.loc[i, 'tweet'])\n",
    "        \n",
    "    def weighted_avg(group, avg_name, weight_name):\n",
    "        \"\"\" calculate weighted avg\n",
    "        \"\"\"\n",
    "        d = group[avg_name]\n",
    "        w = group[weight_name]\n",
    "        try:\n",
    "            return (d * w).sum() / w.sum()\n",
    "        except ZeroDivisionError:\n",
    "            return d.mean()\n",
    "    \n",
    "    if weighted == True:\n",
    "        senti_series = df.groupby(\"date\").apply(weighted_avg, \"senti_score\", \"followers\")\n",
    "        filename = ticker+\"_weighted_sentiment.csv\"\n",
    "        \n",
    "    else:\n",
    "        senti_series = df.groupby(\"date\").mean().senti_score\n",
    "        filename = ticker+\"_avg_sentiment.csv\"\n",
    "\n",
    "    senti = pd.DataFrame({'tweets_date':senti_series.index, 'score':senti_series.values})\n",
    "    senti['day'] = senti['tweets_date'].dt.dayofweek\n",
    "    senti = senti.set_index(\"tweets_date\")\n",
    "    \n",
    "    senti.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Join sentiment with stock movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(ticker, weighted):\n",
    "    \"\"\"join sentiment with stock movement\n",
    "    \"\"\"\n",
    "    stock = pd.read_csv(ticker+\"_stock.csv\")\n",
    "    ### true movement\n",
    "    stock[\"Movement\"] = stock[\"Adj Close\"].pct_change() *100\n",
    "    stock = stock.loc[:, ['Date', 'Movement']]\n",
    "    stock['Date']=pd.to_datetime(stock['Date'], format='%Y-%m-%d') \n",
    "    \n",
    "    import datetime\n",
    "    stock.Date = stock.Date + datetime.timedelta(days = -1)\n",
    "    stock = stock.set_index(\"Date\")\n",
    "    \n",
    "    ####################\n",
    "    ####join dataframes####\n",
    "    ####################   \n",
    "    if weighted == True:\n",
    "        senti = pd.read_csv(ticker+\"_weighted_sentiment.csv\" , index_col=0)\n",
    "    else:\n",
    "        senti = pd.read_csv(ticker+\"_avg_sentiment.csv\", index_col=0)\n",
    "    \n",
    "    result = senti.join(stock, how=\"inner\")\n",
    "    result= result.dropna()\n",
    "    if weighted == True:\n",
    "        result.to_csv(ticker+\"_weighted_joined.csv\")\n",
    "    else:\n",
    "        result.to_csv(ticker+\"_avg_joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(\"MSFT\", weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"MSFT\", weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(\"MSFT\", weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"MSFT\", weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(\"AAPL\", weighted=False)\n",
    "data_processor(\"AAPL\", weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(\"AAPL\", weighted=True)\n",
    "data_processor(\"AAPL\", weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(ticker, weighted):\n",
    "    if weighted == True:        \n",
    "        df = pd.read_csv(ticker+'_weighted_joined.csv', index_col=0)\n",
    "    else:\n",
    "        df = pd.read_csv(ticker+'_avg_joined.csv', index_col=0)\n",
    "    df['Direction'] = df.Movement.apply(lambda x: \"up\" if x>0 else \"down\")\n",
    "    cvar = [\"day\", \"Direction\"]\n",
    "    df[cvar] = df[cvar].astype('category')\n",
    "    df = pd.get_dummies(df, prefix_sep='_')\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    rdummies = [\"day_0\", \"Direction_down\"]\n",
    "    df2 = df2.drop(columns=rdummies)\n",
    "\n",
    "    df3 = df2.copy()\n",
    "    df3 = df3.drop(columns=\"Movement\")\n",
    "    \n",
    "    testpart_size = 0.2\n",
    "    df_nontestData, df_testData = train_test_split(df3, test_size=testpart_size, random_state=1)\n",
    "    \n",
    "    DV = 'Direction_up'\n",
    "    y = df_nontestData[DV]\n",
    "    X = df_nontestData.drop(columns=[DV])\n",
    "    \n",
    "    kfolds = 5\n",
    "    min_alpha = 0.001\n",
    "    max_alpha = 100\n",
    "    n_candidates = 1000\n",
    "    \n",
    "    # C_list is the element-wise inverse of alpha_list. It is required as one of the paramater values for LogisticRegressionCV\n",
    "    C_list = list(1/np.linspace(min_alpha, max_alpha, num=n_candidates))\n",
    "\n",
    "    # Set n_jobs to be -1 to run LogisticRegressionCV on all CPU cores.\n",
    "    clf_optimal = LogisticRegressionCV(Cs=C_list, cv=kfolds, penalty='l1', solver='saga', max_iter=5000, random_state=1, n_jobs=-1).fit(X,y)\n",
    "\n",
    "        # y_test_actual is the actual values of the DV in the test partition\n",
    "    y_test_actual = df_testData[DV]\n",
    "\n",
    "    # X_test is the predictor values in the test partition\n",
    "    X_test = df_testData.drop(columns=[DV])\n",
    "\n",
    "    # Use predict method of the clf_optimal object to apply the model associated with clf_optimal to the test partition\n",
    "    # y_test_predicted is the predicted values of the DV in the test partition \n",
    "    y_test_predicted = clf_optimal.predict(X_test)\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_test_actual, y_test_predicted))\n",
    "    print(clf_optimal.score(X_test, y_test_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"MSFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegression(\"MSFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_processor(\"AAPL\")\n",
    "logisticRegression(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_processor(\"TSLA\")\n",
    "logisticRegression(\"TSLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_processor(\"NIO\")\n",
    "logisticRegression(\"NIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"TSLA2\",weighted=True)\n",
    "logisticRegression(\"TSLA2\",weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"TSLA2\",weighted=False)\n",
    "logisticRegression(\"TSLA2\",weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"AAPL2\",weighted=True)\n",
    "logisticRegression(\"AAPL2\",weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor(\"AAPL2\",weighted=0)\n",
    "logisticRegression(\"AAPL2\",weighted=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
